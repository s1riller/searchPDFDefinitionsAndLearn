{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
    "!unzip uncased_L-12_H-768_A-12.zip\n",
    "!wget https://nlp.stanford.edu/projects/snli/snli_1.0.zip\n",
    "!unzip snli_1.0.zip\n",
    "!wget https://www.nyu.edu/projects/bowman/multinli/multinli_1.0.zip\n",
    "!unzip multinli_1.0.zip\n",
    "\n",
    "!git clone https://github.com/brmson/dataset-sts\n",
    "!git clone https://github.com/gaphex/bert_experimental"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_toxic_data(tox_path):\n",
    "  tox = pd.read_csv(tox_path)\n",
    "  #remove ' ' before and after text\n",
    "  tox['text'] = tox['text'].map(lambda x: str(x).lstrip().rstrip())\n",
    "  #toxic = 1, other = 0\n",
    "  tox['sentiment'] = tox['sentiment'].map(lambda x: 0 if x in ['positive','neutral'] else 1) \n",
    "  toxic_text, toxic_labels = tox.text.values, tox.sentiment.values\n",
    "  return toxic_text, toxic_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ner_data(ner_path, seq_len=24):\n",
    "    data = pd.read_csv(ner_path, encoding= 'unicode_escape', sep=',')\n",
    "    data = data.fillna(method='ffill')\n",
    "    grouped_s = data.groupby('Sentence #', as_index=True)['Word'].apply(lambda g: ' '.join(g))\n",
    "    grouped_t = data.groupby('Sentence #',  as_index=True)['Tag'].apply(lambda g: ' '.join(g))\n",
    "\n",
    "    ner_tr = pd.DataFrame({}, columns=['sentence', 'tag'] )\n",
    "    ner_tr['sentence'] = [st for st in grouped_s.values if len(st.split())<=seq_len]\n",
    "    ner_tr['tag'] = [ tg.split() for tg in grouped_t if len(tg.split())<=seq_len]\n",
    "\n",
    "    tag2idx = {t: i for i,t in enumerate(data.Tag.unique())}\n",
    "    num_tags = len(tag2idx)\n",
    "\n",
    "    y = [[tag2idx[w] for w in s] for s in ner_tr['tag']]\n",
    "    y = pad_sequences(maxlen = seq_len, sequences=y, padding='post', value=tag2idx[\"O\"])\n",
    "\n",
    "    ptargs = [to_categorical(i, num_classes=num_tags) for i in y]        \n",
    "    ptexts = np.array(ner_tr['sentence'])\n",
    "\n",
    "    return ptexts, ptargs, num_tags, tag2idx, ner_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletGenerator:\n",
    "    def __init__(self, datadict, hard_frac = 0.2, batch_size=256):\n",
    "        self.datadict = datadict\n",
    "        self._anchor_idx = np.array(list(self.datadict.keys()))\n",
    "        self._hard_frac = hard_frac\n",
    "        self._generator = self.generate_batch(batch_size)\n",
    "\n",
    "    def generate_batch(self, size):\n",
    "        while True:\n",
    "\n",
    "            hards = int(size*self._hard_frac)\n",
    "            anchor_ids = np.array(np.random.choice(self._anchor_idx, size, replace=False))\n",
    "\n",
    "            anchors = self.get_anchors(anchor_ids)\n",
    "            positives = self.get_positives(anchor_ids)\n",
    "            negatives = np.hstack([self.get_hard_negatives(anchor_ids[:hards]),\n",
    "                                   self.get_random_negatives(anchor_ids[hards:])])\n",
    "            labels = np.ones((size,1))\n",
    "\n",
    "            assert len(anchors) == len(positives) == len(negatives) == len(labels) == size\n",
    "\n",
    "            yield [anchors, positives, negatives], labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MulticlassGenerator:\n",
    "    def __init__(self, data_tuple, batch_size=256):\n",
    "        self._data = data_tuple\n",
    "        self._idx = np.arange(len(data_tuple[-1]))\n",
    "        self.generator = self.generate_batch(batch_size)\n",
    "\n",
    "    def generate_batch(self, size):\n",
    "        while True:\n",
    "            px_ids = np.random.choice(self._idx, size, replace=False)\n",
    "            samples = [p[px_ids] for p in self._data[:-1]]\n",
    "            labels = self._data[-1][px_ids]\n",
    "            \n",
    "            yield samples+[labels], [1]*size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultitaskDataGenerator:\n",
    "    def __init__(self, generators):\n",
    "        self.generators = generators\n",
    "        self.generator = self.generate_batch()\n",
    "        \n",
    "    def generate_batch(self, batch_size=None):\n",
    "        while True:\n",
    "            batch = self.__next__()\n",
    "            yield batch\n",
    "        \n",
    "    def __next__(self):\n",
    "        data_arrays = []\n",
    "        for gen in self.generators:\n",
    "            gen_data, gen_labels = next(gen.generator)\n",
    "\n",
    "            if type(gen_data) not in {list, tuple}:\n",
    "                gen_data = [gen_data]\n",
    "\n",
    "            data_arrays += gen_data\n",
    "\n",
    "        return data_arrays, gen_labels\n",
    "            \n",
    "    def __iter__(self):\n",
    "        return self.__next__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SBERT:\n",
    "    def __init__(self, config):\n",
    "        self.loss = 0\n",
    "        self.metrics = []\n",
    "        self.inputs = []\n",
    "        self.config = config\n",
    "        self.build()\n",
    "        \n",
    "    def build(self):\n",
    "        \n",
    "        self.saver_dict = {}\n",
    "        self.build_body()\n",
    "        \n",
    "        if self.config.use_par_head:\n",
    "            self.build_nli_head()\n",
    "\n",
    "        if self.config.use_toxic_head:\n",
    "            self.build_toxic_head()\n",
    "\n",
    "        if self.config.use_ner_head:\n",
    "            self.build_tag_head()\n",
    "        \n",
    "        self.compile_model()\n",
    "        \n",
    "    def compile_model(self):\n",
    "        \n",
    "        log_this(\"Compiling\")\n",
    "        \n",
    "        self.train_model = tf.keras.models.Model(inputs=self.inputs, outputs=[self.loss])\n",
    "\n",
    "        opt = tf.keras.optimizers.Adam(learning_rate=self.config.lr)\n",
    "        self.train_model.compile(\n",
    "            optimizer=opt,\n",
    "            loss=average_loss,\n",
    "            metrics=self.metrics)\n",
    "        log_this(\"The model is built\")\n",
    "    \n",
    "    def build_body(self):\n",
    "        self.nlu_encoder = BertLayer(\n",
    "            self.config.module_path, self.config.ctx_len,\n",
    "            n_tune_layers=self.config.n_tune, do_preprocessing=True,\n",
    "          pooling='mean', tune_embeddings=self.config.tune_embs,\n",
    "           trainable=self.config.train_bert)\n",
    "\n",
    "    def build_tag_head(self):\n",
    "        \n",
    "        log_this(\"Building tagger head\")\n",
    "        \n",
    "        tag_input = layers.Input(shape=(1, ),  dtype=tf.string)\n",
    "        tag_label = layers.Input(shape=(self.config.ctx_len, self.config.n_tags,), dtype=tf.float32)\n",
    "        \n",
    "        self.nlu_encoder.as_dict = True\n",
    "        inp_tok_encoded =  self.nlu_encoder(tag_input)['token_output']\n",
    "        self.nlu_encoder.as_dict = False\n",
    "        \n",
    "        tag_mlp = self.build_mlp(\n",
    "            2, self.config.dim, self.config.dim, self.config.n_tags, \n",
    "            name=\"ner\", dropout_rate=self.config.head_dropout_rate)\n",
    "        tag_pred = tf.keras.layers.TimeDistributed(tag_mlp)(inp_tok_encoded)\n",
    "        tag_loss = tf.keras.losses.categorical_crossentropy(tag_label, tag_pred)\n",
    "        \n",
    "        self.tag_model = tf.keras.models.Model(inputs=[tag_input], outputs=[tag_pred], name=f'tagger_model')\n",
    "        self.inputs += [tag_input, tag_label]\n",
    "        self.loss += self.config.tagger_loss_weight * tag_loss\n",
    "\n",
    "    def build_nli_head(self):\n",
    "        \n",
    "        log_this(\"Building paraphraser head\")\n",
    "        \n",
    "        anc_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "        pos_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "        neg_input = layers.Input(shape=(1,), dtype=tf.string)\n",
    "\n",
    "        anc_encoded = self.nlu_encoder(anc_input)\n",
    "        pos_encoded = self.nlu_encoder(pos_input)\n",
    "                \n",
    "        if self.config.train_bert:\n",
    "          neg_encoded = self.nlu_encoder(neg_input)\n",
    "          par_loss = tf.keras.layers.Lambda(softmax_loss)([anc_encoded, pos_encoded, neg_encoded])\n",
    "          self.loss += self.config.paraphrase_loss_weight * par_loss\n",
    "        \n",
    "        self.nli_encoder_model = tf.keras.models.Model(inputs=[pos_input], outputs=[pos_encoded])\n",
    "\n",
    "        sim = tf.keras.layers.Lambda(cosine_similarity, name='similarity')([anc_encoded, pos_encoded])\n",
    "        self.sim_model = tf.keras.models.Model(inputs=[anc_input, pos_input], outputs=[sim])\n",
    "        self.inputs += [anc_input, pos_input, neg_input]\n",
    "        \n",
    "        \n",
    "    def build_toxic_head(self):\n",
    "        \n",
    "        log_this(\"Building toxic head\")\n",
    "        \n",
    "        sent_input = layers.Input(shape=(1, ),  dtype=tf.string)\n",
    "        sent_label = layers.Input(shape=(self.config.n_toxic_tags, ), dtype=tf.float32)\n",
    "        \n",
    "        sents_encoded = self.nlu_encoder(sent_input)\n",
    "        \n",
    "        tox_mlp = self.build_mlp(\n",
    "            2, self.config.dim, self.config.dim, self.config.n_toxic_tags, \n",
    "            name=\"toxic\", dropout_rate=self.config.head_dropout_rate)\n",
    "        pred = tox_mlp(sents_encoded)\n",
    "        \n",
    "        tox_loss = tf.keras.losses.categorical_crossentropy(sent_label, pred)\n",
    "        tox_loss = tf.reshape(tox_loss, (-1, 1))\n",
    "        \n",
    "        self.tox_model = tf.keras.models.Model(inputs=[sent_input], outputs=[pred], name=f'toxic_model')\n",
    "        self.inputs += [sent_input, sent_label]\n",
    "        self.loss += self.config.toxic_loss_weight * tox_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = SBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
